CONTAINER_NAME=openmcp

# ==============================
# Docker Container Configuration
# ==============================
HEIGHT=1000
WIDTH=1000


# ==========================
# Ollama Container Settings
# ==========================
OLLAMA_DEBUG=0                       # Enable debug logging (1=on, 0=off)
OLLAMA_HOST=127.0.0.1:11434          # Ollama server bind address
OLLAMA_CONTEXT_LENGTH=4096           # Default model context length (tokens)
OLLAMA_KEEP_ALIVE=24h                # Time models stay loaded in memory


# ======================
# LLM Provider Settings
# ======================
# Supported providers: openai, anthropic
PROVIDER=openai


# ======================
# Available Models List
# ======================
MODELS=(
  "qwen3-vl:8b-instruct"
  "qwen3-vl:235b-a22b-instruct"
  "qwen3-vl:235b"
  "qwen3-vl:32b"
  "qwen3-vl:32b-instruct"
  "qwen3-vl:2b-instruct"
  "ministral-3:8b-instruct-2512-fp16"
  "ministral-3:14b-instruct-2512-fp16"
  "devstral-small-2:24b"
  "seamon67/Gemma3:27b"
  "PetrosStav/gemma3-tools:4b"
  "PetrosStav/gemma3-tools:12b"
  "PetrosStav/gemma3-tools:27b"
  "llama4:17b-scout-16e-instruct-q4_K_M"
  "llama4:17b-scout-16e-instruct-q8_0"
)

# Use this when testing a single model
MODEL=qwen3-vl:8b-instruct-q4_K_M


# ============================
# OpenAI-Compatible API Config
# ============================
# Only used for local LLMs (ignored for anthropic)
OPENAI_BASE_URL=http://host.docker.internal:11434
# OPENAI_BASE_URL=http://129.114.34.174:11434  # Example: remote edge container
OPENAI_ENDPOINT=/v1/chat/completions


# ==================
# Execution Settings
# ==================
# mixed | gui | api
EXEC_MODE=mixed

TASK_TIMEOUT=600        # Per-task execution timeout (seconds)
TOTAL_TIMEOUT=2000      # Total evaluator runtime limit (seconds)
MAX_LLM_CALLS=30        # Max LLM calls per task (blank = unlimited)


# ==================
# LLM Runtime Tuning
# ==================
LLM_TEMPERATURE=0.7     # Higher = more creative, lower = more deterministic

# ==================
# Additional Ollama Container config
# ==================
OLLAMA_MAX_QUEUE=512                 # Max queued requests
OLLAMA_MAX_LOADED_MODELS=1           # Max models loaded per GPU
OLLAMA_MODELS=/var/lib/ollama/models # Path to models directory
OLLAMA_NUM_PARALLEL=1                # Max parallel inference requests
OLLAMA_NOPRUNE=0                     # Disable model blob pruning on startup
OLLAMA_ORIGINS=*                     # Allowed CORS origins
OLLAMA_SCHED_SPREAD=0                # Spread model across all GPUs
OLLAMA_FLASH_ATTENTION=1             # Enable Flash Attention
OLLAMA_KV_CACHE_TYPE=f16             # KV cache quantization type
# OLLAMA_LLM_LIBRARY=                  # Force LLM backend (cuda/rocm/metal)
OLLAMA_GPU_OVERHEAD=0                # Reserved VRAM per GPU (bytes)
OLLAMA_LOAD_TIMEOUT=5m               # Model load timeout
