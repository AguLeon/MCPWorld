# PROVIDER selects the API backend. Supported values: openai, anthropic.
PROVIDER=openai

# MODEL names the LLM to use for the chosen provider.
MODEL=qwen3-vl:32b

# OPENAI_BASE_URL/OPENAI_ENDPOINT point to your OpenAI-compatible server.
OPENAI_BASE_URL=http://host.docker.internal:11434
OPENAI_ENDPOINT=/v1/chat/completions

# EXEC_MODE controls evaluator tool access: mixed, gui, or api.
EXEC_MODE=mixed

# TASK_TIMEOUT caps each task's execution time (seconds).
TASK_TIMEOUT=600
