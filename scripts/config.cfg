# PROVIDER selects the API backend. Supported values: openai, anthropic.
PROVIDER=anthropic

# MODEL names the LLM to use for the chosen provider.
MODEL=claude-3-7-sonnet-20250219      #claude-3-7-sonnet-20250219, qwen3-vl:32b, etc

# OPENAI_BASE_URL/OPENAI_ENDPOINT point to your OpenAI-compatible server.
OPENAI_BASE_URL=http://host.docker.internal:11434
OPENAI_ENDPOINT=/v1/chat/completions

# EXEC_MODE controls evaluator tool access: mixed, gui, or api.
EXEC_MODE=mixed

# TASK_TIMEOUT caps each task's execution time (seconds).
TASK_TIMEOUT=600
