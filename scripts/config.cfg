# PROVIDER selects the API backend protocol. Supported values: openai, anthropic. Most ollama models use openai protocol (such as qwen)
PROVIDER=openai

# MODEL names the LLM to use for the chosen provider.
MODEL=qwen3-vl:2b-instruct     #claude-3-7-sonnet-20250219, qwen3-vl:32b, etc

# OPENAI_BASE_URL/OPENAI_ENDPOINT point to your OpenAI-compatible server.
# No need to change this URL if using 'anthropic'as provider. This is only for local LLM.
OPENAI_BASE_URL=http://host.docker.internal:11434
# OPENAI_BASE_URL=http://129.114.34.174:11434  # To connect with edge container; just use their IPs
OPENAI_ENDPOINT=/v1/chat/completions

# EXEC_MODE controls evaluator tool access: mixed, gui, or api.
EXEC_MODE=mixed

# TASK_TIMEOUT caps each task's execution time (seconds).
TASK_TIMEOUT=300

# TOTAL_TIMEOUT caps the overall wall-clock runtime for each evaluator process
# (including any lingering cleanup). Once reached, the wrapper aborts and moves on.
TOTAL_TIMEOUT=500

# MAX_LLM_CALLS limits how many LLM invocations a task may use. Leave blank to disable.
MAX_LLM_CALLS=30

# LLM Temperature (Higher: more diverse; lower: more deterministic)
LLM_TEMPERATURE=0.7
